{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Installation Command & Importing Libraries"
      ],
      "metadata": {
        "id": "Jp7E1CMkZsuC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install the dataprep library, which is a data preparation library for data scientists.\n",
        "The code snippet installs and imports several libraries and modules that are commonly used in data science and machine learning.\n",
        "pandas is used for data manipulation, plotly.express, matplotlib.pyplot, and seaborn are used for data visualization.\n",
        "LabelEncoder from sklearn.preprocessing is used for encoding categorical variables.\n",
        "train_test_split from sklearn.model_selection is used for splitting data into training and testing sets.\n",
        "mean_squared_error and r2_score from sklearn.metrics are used for evaluating the performance of regression models."
      ],
      "metadata": {
        "id": "4CdOWSQnaFl6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install dataprep\n",
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "#from dataprep.eda import create_report\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, r2_score"
      ],
      "metadata": {
        "id": "jCSjWNE7aClt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reading CSV Files into DataFrames"
      ],
      "metadata": {
        "id": "ZNQTpdVrZPkr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Loading the Features Data:**\n",
        "Reading a CSV file named Features_data_set.csv from the specified Google Drive path using the pandas function read_csv.\n",
        "The data from the CSV file is loaded into a pandas DataFrame named Features_Data.\n",
        "A DataFrame is a two-dimensional, size-mutable, and potentially heterogeneous tabular data structure with labeled axes (rows and columns)."
      ],
      "metadata": {
        "id": "SU0mq6XbakPq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Features_Data = pd.read_csv(\"/content/drive/MyDrive/Data Set/Features_data_set.csv\")"
      ],
      "metadata": {
        "id": "_GZMbxK6ZlJa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Loading the Sales Data:**\n",
        "Reading a CSV file named sales_data_set.csv from the specified Google Drive path using the pandas function read_csv.\n",
        "The data from the CSV file is loaded into a pandas DataFrame named Sales_Data."
      ],
      "metadata": {
        "id": "aQud_Xaea13v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Sales_Data = pd.read_csv(\"/content/drive/MyDrive/Data Set/sales_data_set.csv\")"
      ],
      "metadata": {
        "id": "Yez5ixVZbB3_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Loading the Stores Data:**\n",
        "Reading third CSV file named stores_data_set.csv from the specified Google Drive path using the pandas function read_csv.\n",
        "The data from the CSV file is loaded into a pandas DataFrame named Stores_Data."
      ],
      "metadata": {
        "id": "bfQ0fgdFbEea"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Stores_Data = pd.read_csv(\"/content/drive/MyDrive/Data Set/stores_data_set.csv\")"
      ],
      "metadata": {
        "id": "xcCoMZurbNag"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Purpose:** These lines of code are used to import data from three different CSV files into three separate pandas DataFrames for further analysis or manipulation.\n",
        "**Function Used:** pd.read_csv is a pandas function that reads a comma-separated values (CSV) file into a DataFrame.\n",
        "**File Paths:** The paths provided (/content/drive/MyDrive/Data Set/Features_data_set.csv, /content/drive/MyDrive/Data Set/sales_data_set.csv, /content/drive/MyDrive/Data Set/stores_data_set.csv) are assumed to be locations in Google Drive, typically used when running code on Google Colab.\n",
        "By loading the data into DataFrames, you can now use various pandas functionalities to manipulate, analyze, and visualize the data as needed."
      ],
      "metadata": {
        "id": "lkulAvOobTT4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exploring the DataFrames"
      ],
      "metadata": {
        "id": "Ys0znhtcbsTU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**View the First Few Rows of Features_Data:**\n",
        "The head() method in pandas returns the first 5 rows of the Features_Data DataFrame by default.\n",
        "This is useful to quickly inspect the structure and some initial records of the DataFrame."
      ],
      "metadata": {
        "id": "vjipSZ7JcUBO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Features_Data.head()"
      ],
      "metadata": {
        "id": "rLjYDxPUcOjd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**View the Last Few Rows of Features_Data:**\n",
        "The tail() method in pandas returns the last 5 rows of the Features_Data DataFrame by default.\n",
        "This allows you to see the most recent entries in the DataFrame."
      ],
      "metadata": {
        "id": "n4XSfu69cdmw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Features_Data.tail()"
      ],
      "metadata": {
        "id": "V4V6XkBKckMY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Get a Summary of Features_Data:**\n",
        "The info() method provides a concise summary of the DataFrame, including:\n",
        "The number of non-null entries for each column.\n",
        "The data type of each column.\n",
        "The memory usage of the DataFrame.\n",
        "This is helpful for understanding the general structure and completeness of the data."
      ],
      "metadata": {
        "id": "CF0QxZI1co9T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Features_Data.info()"
      ],
      "metadata": {
        "id": "XHOj8dXncqq9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**View the First Few Rows of Sales_Data:**\n",
        "Similar to Features_Data.head(), this line returns the first 5 rows of the Sales_Data DataFrame."
      ],
      "metadata": {
        "id": "SUnPr1B9ct_9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Sales_Data.head()"
      ],
      "metadata": {
        "id": "WzZlDgp0cyq7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**View the Last Few Rows of Sales_Data:**\n",
        "Similar to Features_Data.tail(), this line returns the last 5 rows of the Sales_Data DataFrame."
      ],
      "metadata": {
        "id": "WyMkn35rc2Uv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Sales_Data.tail()"
      ],
      "metadata": {
        "id": "Fcjo5s2Zc3xN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Get a Summary of Sales_Data:**\n",
        "Similar to Features_Data.info(), this line provides a concise summary of the Sales_Data DataFrame."
      ],
      "metadata": {
        "id": "DXwTEEOic6c4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Sales_Data.info()"
      ],
      "metadata": {
        "id": "D8Bz8btkdBFZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**View the First Few Rows of Stores_Data:**\n",
        "This line returns the first 5 rows of the Stores_Data DataFrame."
      ],
      "metadata": {
        "id": "xLKBhw6edHHF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Stores_Data.head()"
      ],
      "metadata": {
        "id": "mmL0cOh5dIzf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**View the Last Few Rows of Stores_Data:**\n",
        "This line returns the last 5 rows of the Stores_Data DataFrame."
      ],
      "metadata": {
        "id": "Q9o4Lw3fdLSy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Stores_Data.tail()"
      ],
      "metadata": {
        "id": "qfFsq7f7dPvr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Get a Summary of Stores_Data:**\n",
        "Similar to Features_Data.info(), this line provides a concise summary of the Stores_Data DataFrame."
      ],
      "metadata": {
        "id": "kvrct-sndTzC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Stores_Data.info()"
      ],
      "metadata": {
        "id": "E7cE1OqBdUzI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Get Descriptive Statistics of Stores_Data:**\n",
        "The describe() method generates descriptive statistics of the DataFrame, such as:\n",
        "Count,\n",
        "Mean,\n",
        "Standard Deviation,\n",
        "Minimum,\n",
        "25th percentile,\n",
        "Median (50th percentile),\n",
        "75th percentile &\n",
        "Maximum.\n",
        "The \".T\" transposes the result, switching rows and columns for a more readable format.\n",
        "This is useful for understanding the distribution and basic statistics of the numerical columns in the DataFrame."
      ],
      "metadata": {
        "id": "0qEdnt6_dYBX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Stores_Data.describe().T"
      ],
      "metadata": {
        "id": "MRPf-Lwqdt0-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Get Descriptive Statistics of the 'Type' Column in Stores_Data:**\n",
        "The describe() method, when applied to a specific column, generates descriptive statistics for that column.\n",
        "For the 'Type' column, which is likely categorical, it will provide:\n",
        "Count,\n",
        "Unique values count,\n",
        "Top (most frequent) value &\n",
        "Frequency of the top value.\n",
        "The \".T\" transposes the result, although it is more relevant when multiple columns are described together."
      ],
      "metadata": {
        "id": "3ahzjd0Xdvy8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Stores_Data.Type.describe().T"
      ],
      "metadata": {
        "id": "ZPbSAO58d7PJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Purpose:** These lines of code are used to explore and understand the structure, completeness, and basic statistics of the data in each DataFrame.\n",
        "**Methods Used:**\n",
        "**head():** View the first few rows.\n",
        "**tail():** View the last few rows.\n",
        "**info():** Get a summary including data types and non-null counts.\n",
        "**describe():** Generate descriptive statistics.\n",
        "**.T:** Transpose the result for better readability.\n",
        "**Outcome:** These commands help to quickly get an overview of the data, which is essential before proceeding with further analysis or manipulation."
      ],
      "metadata": {
        "id": "aePFqQIrd_X2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Merging DataFrames"
      ],
      "metadata": {
        "id": "qsd8WuWUelEH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Merge Stores_Data and Sales_Data:**\n",
        "**pd.merge():** This function is used to merge two DataFrames.\n",
        "**Stores_Data:** The first DataFrame to be merged.\n",
        "**Sales_Data:** The second DataFrame to be merged.\n",
        "**on='Store':** This specifies that the merge should be done on the 'Store' column, which should be present in both DataFrames.\n",
        "**how='outer':** This specifies the type of merge to be performed. 'outer' merge returns all rows from both DataFrames, with NaNs in places where a row from one DataFrame does not have a corresponding row in the other DataFrame.\n",
        "The result, df, is a new DataFrame that contains all columns from both Stores_Data and Sales_Data, merged on the 'Store' column."
      ],
      "metadata": {
        "id": "kHIxo0tYerMl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.merge(Stores_Data, Sales_Data, on='Store', how='outer')"
      ],
      "metadata": {
        "id": "IKdkPvFde76n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Merge the Resulting DataFrame (df) with Features_Data & View the Last Few Rows of df1:**\n",
        "**pd.merge():** Again, this function is used to merge DataFrames.\n",
        "**df:** The DataFrame resulting from the previous merge (combination of Stores_Data and Sales_Data).\n",
        "**Features_Data:** The third DataFrame to be merged.\n",
        "**on=['Store','Date']:** This specifies that the merge should be done on both the 'Store' and 'Date' columns. Both these columns should be present in the DataFrames being merged.\n",
        "**how='outer':** Similar to the previous merge, this is an 'outer' merge. It ensures that all rows from df and Features_Data are included in the final DataFrame, with NaNs in places where a row from one DataFrame does not have a corresponding row in the other.\n",
        "The result, df1, is a new DataFrame that contains all columns from df and Features_Data, merged on the 'Store' and 'Date' columns.\n",
        "\n",
        "\n",
        "**df1.tail():** This method returns the last 5 rows of the df1 DataFrame by default.\n",
        "This is useful to inspect the most recent entries in the merged DataFrame and verify that the merge operation was performed correctly."
      ],
      "metadata": {
        "id": "6eZ4JBsQe8ve"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df1 = pd.merge(df, Features_Data, on=['Store','Date'], how='outer')\n",
        "df1.tail()"
      ],
      "metadata": {
        "id": "0MMqPHpTfPA8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**First Merge:** Combines Stores_Data and Sales_Data based on the 'Store' column.\n",
        "The result (df) contains all rows from both DataFrames with NaNs where there are mismatches.\n",
        "\n",
        "**Second Merge:** Combines df (the result of the first merge) with Features_Data based on both 'Store' and 'Date' columns.\n",
        "The result (df1) includes all rows from both DataFrames with NaNs where there are mismatches.\n",
        "\n",
        "**Inspecting the Result:** df1.tail(): Shows the last few rows of the merged DataFrame df1 to verify the merge operation.\n",
        "\n",
        "These steps ensure that all relevant data from Stores_Data, Sales_Data, and Features_Data are combined into a single DataFrame (df1), which can then be used for further analysis or processing"
      ],
      "metadata": {
        "id": "5kjR9BblfkR8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Converting Date Column to DateTime Format"
      ],
      "metadata": {
        "id": "O2fUfqimgbPo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Convert 'Date' Column to DateTime:**\n",
        "pd.to_datetime(): This function converts a column to datetime format.\n",
        "df1.Date: The 'Date' column in the df1 DataFrame.\n",
        "format=\"%d/%m/%Y\": Specifies the format of the date strings in the 'Date' column (day/month/year).\n",
        "This line converts the 'Date' column from string format to pandas datetime format, which makes it easier to perform date-related operations."
      ],
      "metadata": {
        "id": "_dextq0Zghsp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df1.Date = pd.to_datetime(df1.Date, format=\"%d/%m/%Y\")"
      ],
      "metadata": {
        "id": "Jj0rrGWZgdoN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**View the First Few Rows of df1:**\n",
        "df1.head(): This method returns the first 5 rows of the df1 DataFrame.\n",
        "This is useful for verifying that the date conversion has been applied correctly and inspecting the initial rows of the DataFrame."
      ],
      "metadata": {
        "id": "_KNuSVTdgprF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df1.head()"
      ],
      "metadata": {
        "id": "Nkd4p3Icgsji"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**View Unique Dates:**\n",
        "df1.Date.unique(): This method returns an array of unique values in the 'Date' column.\n",
        "This is useful for checking the range and distinct values of dates in the DataFrame."
      ],
      "metadata": {
        "id": "Zb-K-XMsgwCY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df1.Date.unique()"
      ],
      "metadata": {
        "id": "T2EqrtrYgyY6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Drop Rows with Dates After November 1, 2012:**\n",
        "df1['Date'] > '2012-11-01': This condition checks each row's 'Date' column to see if it is greater than '2012-11-01'.\n",
        "df1[df1['Date'] > '2012-11-01'].index: This extracts the indices of rows where the condition is true.\n",
        "df1.drop(..., inplace=True): This drops the rows with the specified indices from the DataFrame and inplace=True means the operation is performed on the original DataFrame (df1) without returning a new DataFrame.\n",
        "This line removes all rows from df1 where the date is later than November 1, 2012."
      ],
      "metadata": {
        "id": "6k9Yv8Pjg1Ew"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df1.drop(df1[df1['Date'] > '2012-11-01'].index, inplace=True)"
      ],
      "metadata": {
        "id": "38QaRRJgg2tt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**View the Last Few Rows of df1:**\n",
        "df1.tail(): This method returns the last 5 rows of the df1 DataFrame.\n",
        "This is useful for verifying that the rows with dates after November 1, 2012, have been successfully removed and inspecting the final rows of the DataFrame."
      ],
      "metadata": {
        "id": "179OFgPTg8oo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df1.tail()"
      ],
      "metadata": {
        "id": "1HCDAHD7g-zI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**View Unique Dates Again:**\n",
        "df1.Date.unique(): This method returns an array of unique values in the 'Date' column.\n",
        "This is useful for verifying the range and distinct values of dates in the DataFrame after the rows with dates later than November 1, 2012, have been dropped."
      ],
      "metadata": {
        "id": "nsRu9zRHhA7V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df1.Date.unique()"
      ],
      "metadata": {
        "id": "dRFyq-2JhDPy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Convert 'Date' Column:** Converts the 'Date' column from string to datetime format.\n",
        "**View Initial Rows:** Inspects the first few rows to verify the conversion.\n",
        "**Check Unique Dates:** Lists unique dates to understand the range and distinct values.\n",
        "**Drop Future Dates:** Removes rows where the date is later than November 1, 2012.\n",
        "**View Final Rows:** Inspects the last few rows to confirm the rows have been dropped.\n",
        "**Check Unique Dates Again:** Verifies the range and distinct values of dates after dropping rows.\n",
        "These steps ensure that the DataFrame df1 has the 'Date' column in the correct format and only includes data up to November 1, 2012. This is important for maintaining a consistent and relevant dataset for further analysis."
      ],
      "metadata": {
        "id": "XWu4WY4khFqy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exploratory Data Analysis (EDA)"
      ],
      "metadata": {
        "id": "TTmt5aDHh8e2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "View the First Few Rows of df1.\n",
        "View the Last Few Rows of df1.\n",
        "Get DataFrame Information.\n",
        "List Column Names."
      ],
      "metadata": {
        "id": "gATf_x-6h_EO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df1.head()\n",
        "df1.tail()\n",
        "df1.info()\n",
        "df1.columns"
      ],
      "metadata": {
        "id": "rE1FB4EWiFov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define Continuous Columns:**\n",
        "A list of column names that represent continuous numerical data.\n",
        "These are columns that will likely be used for numerical analysis and statistical operations."
      ],
      "metadata": {
        "id": "IuFA3kitic3u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "continuous_columns = ['Size', 'Weekly_Sales', 'Temperature', 'Fuel_Price', 'MarkDown1', 'MarkDown2',\n",
        "                      'MarkDown3', 'MarkDown4', 'MarkDown5', 'CPI', 'Unemployment']"
      ],
      "metadata": {
        "id": "yfY8qkRFid6r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define Categorical Columns:**\n",
        "A list of column names that represent categorical data.\n",
        "These columns are typically used for grouping, aggregation, and categorical analysis."
      ],
      "metadata": {
        "id": "rDDSHC3uilhY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "category_columns = [\"Store\",  \"Type\", \"IsHoliday_x\", \"IsHoliday_y\", \"Dept\"]"
      ],
      "metadata": {
        "id": "xcic0anGipT2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Select String Columns:**\n",
        "df1.select_dtypes(exclude=['int64', \"float64\", \"datetime64\"]): Selects columns that are not of types int64, float64, or datetime64.\n",
        ".columns: Gets the column names of the selected columns.\n",
        "This identifies columns that are likely to be of object/string type."
      ],
      "metadata": {
        "id": "0BI-A2W-isIU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "string_columns = df1.select_dtypes(exclude=['int64', \"float64\", \"datetime64\"]).columns"
      ],
      "metadata": {
        "id": "D6ZKaeMVizeF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Select Numeric Columns:**\n",
        "df1.select_dtypes(include=['int64', \"float64\"]): Selects columns that are of types int64 or float64.\n",
        ".columns: Gets the column names of the selected columns.\n",
        "This identifies columns that are numerical."
      ],
      "metadata": {
        "id": "7uhCFT2ai5bG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "numeric_columns = df1.select_dtypes(include=['int64', \"float64\"]).columns"
      ],
      "metadata": {
        "id": "8SHrG62Ii6ua"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Descriptive Statistics for Numeric Columns:**\n",
        "df1.describe(): This method generates descriptive statistics for numeric columns (e.g., count, mean, std, min, max, and percentiles).\n",
        ".T: Transposes the DataFrame for a better view (columns become rows).\n",
        "Provides a statistical summary of the numerical columns in the DataFrame."
      ],
      "metadata": {
        "id": "p8IH9k0OjMw3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df1.describe().T"
      ],
      "metadata": {
        "id": "L4BypNHejOP6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Descriptive Statistics for String Columns:**\n",
        "df1[string_columns].describe(): Generates descriptive statistics for string columns (e.g., count, unique, top, freq).\n",
        ".T: Transposes the DataFrame for a better view (columns become rows).\n",
        "Provides a summary of the categorical/string columns."
      ],
      "metadata": {
        "id": "x1sEGNumjRea"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df1[string_columns].describe().T"
      ],
      "metadata": {
        "id": "vC3Jm5zXjVSA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Count Missing Values:**\n",
        "df1.isnull(): Checks each element of the DataFrame for missing values (returns a DataFrame of the same shape with True for missing values and False otherwise).\n",
        ".sum(): Sums the True values for each column (True is treated as 1).\n",
        "This line provides the number of missing values in each column, helping identify columns that need data cleaning or imputation."
      ],
      "metadata": {
        "id": "oY8q-jZNjcHu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df1.isnull().sum()"
      ],
      "metadata": {
        "id": "x2xOzGoQjhbk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**View Data:** head(), tail(), info() and columns help understand the structure, data types, and initial entries of the DataFrame.\n",
        "**Define Column Categories:** continuous_columns and category_columns categorize the columns for easier analysis.\n",
        "**Select Columns by Data Type:** select_dtypes helps identify string and numeric columns separately.\n",
        "**Descriptive Statistics:** describe().T gives a summary of numeric and string columns.\n",
        "**Check Missing Values:** isnull().sum() identifies columns with missing values, guiding the need for data cleaning.\n",
        "These steps are foundational for EDA, helping you understand your dataset's structure, contents, and any potential issues such as missing values."
      ],
      "metadata": {
        "id": "bSmbyvPwjn-Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Correlation Matrix for Numeric Columns"
      ],
      "metadata": {
        "id": "fme-mCRVlFeJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Calculate Correlation Matrix for Numeric Columns:**\n",
        "df1[numeric_columns].corr(): This computes the pairwise correlation of columns specified in numeric_columns.\n",
        "Correlation is a statistical measure that indicates the extent to which two variables fluctuate together."
      ],
      "metadata": {
        "id": "i_ytPQa7lJj6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df1[numeric_columns].corr()"
      ],
      "metadata": {
        "id": "bOBY3IbQlPXC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Display String Columns:**\n",
        "string_columns: This simply outputs the list of string columns identified earlier.\n",
        "This helps verify which columns are considered string-type for further processing."
      ],
      "metadata": {
        "id": "PS8MVLZnlTyH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "string_columns"
      ],
      "metadata": {
        "id": "csMaJet8lU2m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create a Copy of df1:**\n",
        "df2 = df1.copy(): This creates a copy of df1 named df2.\n",
        "It ensures that any operations performed on df2 do not affect the original DataFrame df1."
      ],
      "metadata": {
        "id": "zBI_eX-QlYA7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df2 = df1.copy()"
      ],
      "metadata": {
        "id": "EARpFRcOlbME"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Initialize Label Encoder:**\n",
        "encode = LabelEncoder(): This initializes a LabelEncoder instance from sklearn.preprocessing.\n",
        "Label encoding converts categorical string data into numerical data, which is necessary for many machine learning algorithms."
      ],
      "metadata": {
        "id": "I0PfewfxldRZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encode = LabelEncoder()"
      ],
      "metadata": {
        "id": "jPsK1dm4lfdx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Encode Each String Column:**\n",
        "This loop iterates over each column in string_columns.\n",
        "encode.fit(df2[[column]]): Fits the encoder to the unique values in the column.\n",
        "df2[column] = encode.transform(df2[[column]]): Transforms the original string values into numeric values."
      ],
      "metadata": {
        "id": "azRheQlsljgq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for column in string_columns:\n",
        "  encode.fit(df2[[column]])\n",
        "  df2[column] = encode.transform(df2[[column]])"
      ],
      "metadata": {
        "id": "i8dC6onylkzc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Display the First Few Rows of df2:**\n",
        "df2.head(): Shows the first 5 rows of the DataFrame df2 to verify the encoding process."
      ],
      "metadata": {
        "id": "4QWVOH7slrgo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df2.head()"
      ],
      "metadata": {
        "id": "bV7iStu6lt52"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create and Display Correlation Matrix Plot:**\n",
        "correlation_matrix = df2.corr(): Computes the correlation matrix for df2.\n",
        "fig = px.imshow(correlation_matrix, color_continuous_scale='Viridis', title=\"Correlation Matrix\"): Uses Plotly Express to create a heatmap of the correlation matrix.\n",
        "fig.show(): Displays the heatmap."
      ],
      "metadata": {
        "id": "p13heZ2-lwle"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "correlation_matrix = df2.corr()\n",
        "fig = px.imshow(correlation_matrix, color_continuous_scale='Viridis', title=\"Correlation Matrix\")\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "wmqSRpEMlxx7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Generate EDA Report (Commented Out):**\n",
        "create_report(df1): This would generate an EDA report using dataprep.eda (if uncommented).\n",
        "It provides comprehensive insights into the DataFrame, including data distributions, missing values, and correlations."
      ],
      "metadata": {
        "id": "Z7ZDZu72lzuH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#create_report(df1)"
      ],
      "metadata": {
        "id": "vCeKSME-l38t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define Function to Plot Box Plot and Histogram:**\n",
        "def plot(df, column): Defines a function plot that takes a DataFrame df and a column name column.\n",
        "plt.figure(figsize=(20,5)): Creates a new figure with specified size.\n",
        "plt.subplot(1,2,1): Creates the first subplot in a 1x2 grid.\n",
        "sns.boxplot(data=df, x=column): Creates a box plot for the specified column.\n",
        "plt.title(f'Box Plot for {column}'): Sets the title for the box plot.\n",
        "plt.subplot(1,2,2): Creates the second subplot in a 1x2 grid.\n",
        "sns.histplot(data=df, x=column, kde=True, bins=50): Creates a histogram with a KDE (Kernel Density Estimate) for the specified column.\n",
        "plt.title(f'Distribution Plot for {column}'): Sets the title for the histogram."
      ],
      "metadata": {
        "id": "6S32PDkKl_bJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot(df, column):\n",
        "    plt.figure(figsize=(20,5))\n",
        "    plt.subplot(1,2,1)\n",
        "    sns.boxplot(data=df, x=column)\n",
        "    plt.title(f'Box Plot for {column}')\n",
        "\n",
        "    plt.subplot(1,2,2)\n",
        "    sns.histplot(data=df, x=column, kde=True, bins=50)\n",
        "    plt.title(f'Distribution Plot for {column}')"
      ],
      "metadata": {
        "id": "cOpx1sICmCpJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Plot for Each Numeric Column:**\n",
        "This loop iterates over each column in numeric_columns.\n",
        "plot(df1, i): Calls the plot function for each column, creating a box plot and histogram."
      ],
      "metadata": {
        "id": "mUZ7BoVemFLm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in numeric_columns:\n",
        "    plot(df1, i)"
      ],
      "metadata": {
        "id": "aE3ozrmRmGRI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Summary**\n",
        "\n",
        "**Calculate Correlations:** The correlation matrix for numeric columns helps understand the relationships between variables.\n",
        "**Label Encoding:** Encodes categorical string data into numerical format, making it suitable for analysis and machine learning.\n",
        "**Correlation Plot:** Visualizes correlations using a heatmap, aiding in identifying strong relationships between features.\n",
        "**EDA Report:** (commented out) Provides an automated, comprehensive analysis of the data.\n",
        "**Plotting Function:** Creates box plots and histograms for visualizing the distribution and spread of numeric data.\n",
        "**Iterate Over Numeric Columns:** Generates plots for each numeric column, offering insights into the data's distribution and potential outliers."
      ],
      "metadata": {
        "id": "E_1AisJgmJzM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Pre-Processing"
      ],
      "metadata": {
        "id": "35H7NhK7oKSr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Display the First Few Rows of df1\n",
        "\n",
        "Check for Duplicates\n",
        "\n",
        "Count Unique Values for 'IsHoliday_x':\n",
        "df1.IsHoliday_x.value_counts(): Counts the occurrences of each unique value in the 'IsHoliday_x' column.\n",
        "\n",
        "Count Unique Values for 'IsHoliday_y':\n",
        "df1.IsHoliday_y.value_counts(): Counts the occurrences of each unique value in the 'IsHoliday_y' column.\n",
        "\n",
        "Drop 'IsHoliday_x' Column:\n",
        "df1= df1.drop(columns=['IsHoliday_x']): Removes the 'IsHoliday_x' column from the DataFrame.\n",
        "\n",
        "Check Shape of DataFrame:\n",
        "df1.shape: Outputs the shape (number of rows and columns) of df1.\n",
        "\n",
        "Check for Missing Values:\n",
        "df1.isnull().sum(): Counts the number of missing values in each column of df1."
      ],
      "metadata": {
        "id": "YKtzYnt4oQMx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df1.head()\n",
        "df1.duplicated().sum()\n",
        "df1.IsHoliday_x.value_counts()\n",
        "df1.IsHoliday_y.value_counts()\n",
        "df1= df1.drop(columns=['IsHoliday_x'])\n",
        "df1.shape\n",
        "df1.isnull().sum()"
      ],
      "metadata": {
        "id": "rnhUjy0OoklN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Calculate Proportion of Missing Values:**\n",
        "These lines calculate the proportion of missing values in the 'MarkDown' columns by dividing the number of missing values by the total number of rows (421,570).\n",
        "print(Markdown1,Markdown2,Markdown3,Markdown4,Markdown5): Prints the calculated proportions."
      ],
      "metadata": {
        "id": "tpoXTikAo-0n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Markdown1=270889/421570\n",
        "Markdown2=310322/421570\n",
        "Markdown3=284479/421570\n",
        "Markdown4=286603/421570\n",
        "Markdown5=270138/421570\n",
        "print(Markdown1,Markdown2,Markdown3,Markdown4,Markdown5)"
      ],
      "metadata": {
        "id": "sqRF_UJkpBPj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Fill Missing Values with Zero:**\n",
        "For each 'MarkDown' column, missing values are replaced with 0 using the fillna method with inplace=True."
      ],
      "metadata": {
        "id": "nB6rOe-spEhs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df1.MarkDown1.fillna(0, inplace = True)\n",
        "df1.MarkDown2.fillna(0, inplace = True)\n",
        "df1.MarkDown3.fillna(0, inplace = True)\n",
        "df1.MarkDown4.fillna(0, inplace = True)\n",
        "df1.MarkDown5.fillna(0, inplace = True)"
      ],
      "metadata": {
        "id": "QPZ0zfXlpHT3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Rechecking for missing values again\n",
        "df1.isnull().sum()"
      ],
      "metadata": {
        "id": "BRTxPk1FpLxy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Extract Date Components:**\n",
        "df1['month'] = df1['Date'].dt.month: Extracts the month from the 'Date' column and creates a new column 'month'.\n",
        "df1['year'] = df1['Date'].dt.year: Extracts the year from the 'Date' column and creates a new column 'year'.\n",
        "df1['day'] = df1['Date'].dt.day: Extracts the day from the 'Date' column and creates a new column 'day'.\n",
        "df1.head(): Displays the first 5 rows of df1 to verify the new columns."
      ],
      "metadata": {
        "id": "egM6qRskpSo2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df1['month'] = df1['Date'].dt.month\n",
        "df1['year'] = df1['Date'].dt.year\n",
        "df1['day'] = df1['Date'].dt.day\n",
        "df1.head()"
      ],
      "metadata": {
        "id": "UgjygwVHpVfs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Drop 'Date' Column:**\n",
        "df1= df1.drop(columns=['Date']): Removes the 'Date' column from the DataFrame since its components have been extracted."
      ],
      "metadata": {
        "id": "ETCbMDgXpYGl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df1= df1.drop(columns=['Date'])"
      ],
      "metadata": {
        "id": "M0507zaJpai0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define Outlier Handling Function:**\n",
        "def outlier(df, column): Defines a function outlier that takes a DataFrame df and a column name column.\n",
        "\n",
        "iqr = df[column].quantile(0.75) - df[column].quantile(0.25): Calculates the Interquartile Range (IQR) for the column.\n",
        "\n",
        "upper_threshold = df[column].quantile(0.75) + (1.5*iqr): Calculates the upper threshold for detecting outliers.\n",
        "\n",
        "lower_threshold = df[column].quantile(0.25) - (1.5*iqr): Calculates the lower threshold for detecting outliers.\n",
        "\n",
        "df[column] = df[column].clip(lower_threshold, upper_threshold): Clips the column values to be within the thresholds."
      ],
      "metadata": {
        "id": "oyxF6Ypgpdt7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def outlier(df, column):\n",
        "    iqr = df[column].quantile(0.75) - df[column].quantile(0.25)\n",
        "    upper_threshold = df[column].quantile(0.75) + (1.5*iqr)\n",
        "    lower_threshold = df[column].quantile(0.25) - (1.5*iqr)\n",
        "    df[column] = df[column].clip(lower_threshold, upper_threshold)"
      ],
      "metadata": {
        "id": "fgzzFECUplLo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Handle Outliers and Plot:**\n",
        "For each specified column, the outlier function is called to clip the values within thresholds.\n",
        "After handling outliers, the plot function (defined earlier) is called to create box plots and histograms for each column.\n",
        "This process is repeated for 'Unemployment', 'Temperature', 'MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', and 'MarkDown5'."
      ],
      "metadata": {
        "id": "YuarZ_YGpqix"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "outlier(df1, 'Unemployment')\n",
        "plot(df1, 'Unemployment')\n",
        "\n",
        "outlier(df1, 'Temperature')\n",
        "plot(df1, 'Temperature')\n",
        "\n",
        "outlier(df1, 'MarkDown1')\n",
        "plot(df1, 'MarkDown1')\n",
        "\n",
        "outlier(df1, 'MarkDown2')\n",
        "plot(df1, 'MarkDown2')\n",
        "\n",
        "outlier(df1, 'MarkDown3')\n",
        "plot(df1, 'MarkDown3')\n",
        "\n",
        "outlier(df1, 'MarkDown4')\n",
        "plot(df1, 'MarkDown4')\n",
        "\n",
        "outlier(df1, 'MarkDown5')\n",
        "plot(df1, 'MarkDown5')"
      ],
      "metadata": {
        "id": "Rgq7fbb3ptJs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Calculate IQR and Thresholds for 'Weekly_Sales':**\n",
        "Calculates the IQR for 'Weekly_Sales'.\n",
        "Computes the upper and lower thresholds for outlier detection.\n",
        "Outputs the IQR, upper threshold, and lower threshold values."
      ],
      "metadata": {
        "id": "CDiswOLfpuDg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "iqr = df1['Weekly_Sales'].quantile(0.75) - df1['Weekly_Sales'].quantile(0.25)\n",
        "upper_threshold = df1['Weekly_Sales'].quantile(0.75) + (1.5*iqr)\n",
        "lower_threshold = df1['Weekly_Sales'].quantile(0.25) - (1.5*iqr)\n",
        "iqr, upper_threshold, lower_threshold"
      ],
      "metadata": {
        "id": "RM9yj5mcpxgo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Describe 'Weekly_Sales':**\n",
        "df1.Weekly_Sales.describe(): Outputs summary statistics for the 'Weekly_Sales' column, including count, mean, std, min, 25%, 50%, 75%, and max."
      ],
      "metadata": {
        "id": "3tr6Na-Gp17q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df1.Weekly_Sales.describe()"
      ],
      "metadata": {
        "id": "9LjZIHilp4OB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Plot 'Weekly_Sales':**\n",
        "Calls the plot function to create a box plot and histogram for 'Weekly_Sales'."
      ],
      "metadata": {
        "id": "o3g1USwKp6zp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot(df1, 'Weekly_Sales')"
      ],
      "metadata": {
        "id": "NQS42yeLp7yZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Count Negative 'Weekly_Sales' Values:**\n",
        "\n",
        "len(df1[df1['Weekly_Sales'] < 0]): Counts the number of rows where 'Weekly_Sales' is negative.\n",
        "Count Extremely High 'Weekly_Sales' Values:\n",
        "\n",
        "len(df1[df1['Weekly_Sales'] >= 250000]): Counts the number of rows where 'Weekly_Sales' is greater than or equal to 250,000."
      ],
      "metadata": {
        "id": "Q8cPLZ2dqBqc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(df1[df1['Weekly_Sales'] < 0])\n",
        "\n",
        "len(df1[df1['Weekly_Sales'] >= 250000])"
      ],
      "metadata": {
        "id": "gIHssIX6p_zu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Shape of DataFrame\n",
        "df1.shape"
      ],
      "metadata": {
        "id": "_xMjjuqVqHYU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Drop Rows with Negative 'Weekly_Sales' Values:**\n",
        "\n",
        "df1.drop(df1[df1['Weekly_Sales'] < 0].index, inplace=True): Drops rows where 'Weekly_Sales' is negative.\n",
        "Drop Rows with Extremely High 'Weekly_Sales' Values:\n",
        "\n",
        "df1.drop(df1[df1['Weekly_Sales'] >= 250000].index, inplace=True): Drops rows where 'Weekly_Sales' is greater than or equal to 250,000."
      ],
      "metadata": {
        "id": "Bpi1wAvBqOg2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df1.drop(df1[df1['Weekly_Sales'] < 0].index, inplace=True)\n",
        "df1.drop(df1[df1['Weekly_Sales'] >= 250000].index, inplace=True)"
      ],
      "metadata": {
        "id": "DPlqcuONqROz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Re-check Shape of DataFrame\n",
        "df1.shape\n",
        "\n",
        "# Re-plot 'Weekly_Sales'\n",
        "plot(df1, 'Weekly_Sales')"
      ],
      "metadata": {
        "id": "w2f6D-ClqU1x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Summary**\n",
        "\n",
        "**Initial Inspection:** Displayed first few rows, checked for duplicates, and value counts for holiday indicators.\n",
        "**Cleaning and Preprocessing:** Removed 'IsHoliday_x', checked shape, and missing values.\n",
        "**Missing Values Handling:** Filled missing 'MarkDown' values with zeros.\n",
        "**Date Feature Engineering:** Extracted month, year, and day from 'Date' and then dropped 'Date' column.\n",
        "**Outlier Handling:** Defined and applied a function to clip outliers for multiple columns.\n",
        "**'Weekly_Sales' Analysis:** Calculated IQR and thresholds, described, plotted, and removed extreme values, then re-checked the shape and plotted again."
      ],
      "metadata": {
        "id": "QDTpeL3aq3IT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Encoding"
      ],
      "metadata": {
        "id": "iCK3g6PZrWzs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the First Two Rows of df1\n",
        "df1.head(2)"
      ],
      "metadata": {
        "id": "r_MWusVHrgVR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define Columns to Encode:**\n",
        "columns=[\"Type\", \"IsHoliday_y\"]: Creates a list named columns containing the names of the columns that will be encoded. In this case, 'Type' and 'IsHoliday_y'.\n",
        "\n",
        "**Initialize the LabelEncoder:**\n",
        "encode=LabelEncoder(): Initializes an instance of the LabelEncoder from scikit-learn, which will be used to convert categorical labels into numeric form."
      ],
      "metadata": {
        "id": "2N-Xf7kGrrWy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "columns=[\"Type\", \"IsHoliday_y\"]\n",
        "encode=LabelEncoder()"
      ],
      "metadata": {
        "id": "MhHFwZPhr1m8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Loop Through Each Column in the List:**\n",
        "\n",
        "for column in columns:: Starts a loop that will iterate over each column name in the columns list.\n",
        "Fit the Encoder to the Column:\n",
        "\n",
        "encode.fit(df1[[column]]): Fits the LabelEncoder to the column data. This step learns the unique values in the column and assigns a numeric label to each unique value.\n",
        "Note: Typically, LabelEncoder's fit method expects a 1-dimensional array, but here it's being called with a DataFrame. This might work with some versions but ideally should be encode.fit(df1[column]).\n",
        "Transform the Column Data:\n",
        "\n",
        "df1[column] = encode.transform(df1[[column]]): Transforms the categorical data in the column into numeric labels. The transformed numeric values replace the original values in the DataFrame.\n",
        "Similar to the fit method, transform is typically used with a 1-dimensional array: df1[column] = encode.transform(df1[column])."
      ],
      "metadata": {
        "id": "HrVkdfnYr7MN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for column in columns:\n",
        "  encode.fit(df1[[column]])\n",
        "  df1[column] = encode.transform(df1[[column]])"
      ],
      "metadata": {
        "id": "3smlhRnUr8s2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the First Two Rows of df1 After Encoding\n",
        "df1.head(2)"
      ],
      "metadata": {
        "id": "Bn08sqbdsY6D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Summary**\n",
        "\n",
        "**Initial Display: **The first two rows of df1 are displayed to show the original data.\n",
        "**Columns to Encode:** A list of columns to be encoded is defined.\n",
        "**Label Encoder Initialization:** A LabelEncoder object is initialized.\n",
        "**Encoding Loop:** For each column in the list, the encoder is fitted to the column's data, and the column's categorical values are transformed into numeric labels.\n",
        "**Final Display:** The first two rows of df1 are displayed again to show the transformed numeric values in the specified columns."
      ],
      "metadata": {
        "id": "bVhzxQocsdqh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Machine Learning"
      ],
      "metadata": {
        "id": "cUG-xu_RtKKx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define Features and Target Variable:**\n",
        "\n",
        "x: Contains the feature variables by dropping the \"Weekly_Sales\" column from the DataFrame df1.\n",
        "\n",
        "y: Contains the target variable \"Weekly_Sales\".\n",
        "Split the Data into Training and Testing Sets:\n",
        "\n",
        "train_test_split(x, y, test_size=0.25, random_state=42): Splits the dataset into training and testing sets. 75% of the data will be used for training (x_train and y_train), and 25% will be used for testing (x_test and y_test). The random_state parameter ensures reproducibility by fixing the random seed."
      ],
      "metadata": {
        "id": "yCi3aejGtNWH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x=df1.drop(\"Weekly_Sales\",axis=1)\n",
        "y=df1[\"Weekly_Sales\"]\n",
        "\n",
        "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.25, random_state=42)"
      ],
      "metadata": {
        "id": "CGVKmmWftWGR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Initialize and Train the SVR Model:**\n",
        "\n",
        "SVR(): Initializes a Support Vector Regression (SVR) model.\n",
        ".fit(x_train, y_train): Fits the SVR model to the training data (x_train and y_train).\n",
        "\n",
        "Make Predictions:\n",
        "y_pred_train and y_pred_test: Predicts the target variable for both the training and testing datasets.\n",
        "\n",
        "Evaluate the Model:\n",
        "Calculates the coefficient of determination (R^2) for both the training and testing sets."
      ],
      "metadata": {
        "id": "cqcMGYgotgPA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVR\n",
        "Model2=SVR().fit(x_train,y_train)\n",
        "y_pred_train = Model2.predict(x_train)\n",
        "y_pred_test = Model2.predict(x_test)\n",
        "\n",
        "r2_train = r2_score(y_train, y_pred_train)\n",
        "r2_test = r2_score(y_test, y_pred_test)\n",
        "\n",
        "r2_train, r2_test"
      ],
      "metadata": {
        "id": "DX5XyN32tlm-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Initialize and Train the Decision Tree Regressor Model:**\n",
        "\n",
        "DecisionTreeRegressor(): Initializes a Decision Tree Regressor model.\n",
        "\n",
        ".fit(x_train, y_train): Fits the Decision Tree Regressor model to the training data (x_train and y_train).\n",
        "\n",
        "Make Predictions and Evaluate the Model:\n",
        "Same as the SVR model, predictions are made and the R^2 scores are calculated for both the training and testing datasets."
      ],
      "metadata": {
        "id": "-tJUj4Fqtudp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeRegressor\n",
        "Model4=DecisionTreeRegressor().fit(x_train,y_train)\n",
        "y_pred_train = Model4.predict(x_train)\n",
        "y_pred_test = Model4.predict(x_test)\n",
        "\n",
        "r2_train = r2_score(y_train, y_pred_train)\n",
        "r2_test = r2_score(y_test, y_pred_test)\n",
        "\n",
        "r2_train, r2_test"
      ],
      "metadata": {
        "id": "YXWEkyGPt7JJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Initialize and Train the Gradient Boosting Regressor Model:**\n",
        "\n",
        "GradientBoostingRegressor(): Initializes a Gradient Boosting Regressor model.\n",
        "\n",
        ".fit(x_train, y_train): Fits the Gradient Boosting Regressor model to the training data (x_train and y_train).\n",
        "\n",
        "Make Predictions and Evaluate the Model:\n",
        "Predictions are made and the R^2 scores are calculated for both the training and testing datasets."
      ],
      "metadata": {
        "id": "cgAnQ3RauB_a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "Model6=GradientBoostingRegressor().fit(x_train,y_train)\n",
        "y_pred_train = Model6.predict(x_train)\n",
        "y_pred_test = Model6.predict(x_test)\n",
        "\n",
        "r2_train = r2_score(y_train, y_pred_train)\n",
        "r2_test = r2_score(y_test, y_pred_test)\n",
        "\n",
        "r2_train, r2_test"
      ],
      "metadata": {
        "id": "8lFBuBZmuKmt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Initialize and Train the Random Forest Regressor Model:**\n",
        "\n",
        "RandomForestRegressor(): Initializes a Random Forest Regressor model.\n",
        "\n",
        ".fit(x_train, y_train): Fits the Random Forest Regressor model to the training data (x_train and y_train).\n",
        "\n",
        "Make Predictions and Evaluate the Model:\n",
        "Predictions are made and the R^2 scores are calculated for both the training and testing datasets."
      ],
      "metadata": {
        "id": "eFIN2aBmuR0_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "Model5=RandomForestRegressor().fit(x_train,y_train)\n",
        "y_pred_train = Model5.predict(x_train)\n",
        "y_pred_test = Model5.predict(x_test)\n",
        "\n",
        "r2_train = r2_score(y_train, y_pred_train)\n",
        "r2_test = r2_score(y_test, y_pred_test)\n",
        "\n",
        "r2_train, r2_test"
      ],
      "metadata": {
        "id": "pvCCcKf8uXY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Calculate Mean Squared Error (MSE):**\n",
        "Calculates the MSE between the true target values (y_test) and the predicted target values (y_pred_test)."
      ],
      "metadata": {
        "id": "zfCj9H9QucWY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mse = mean_squared_error(y_test, y_pred_test)\n",
        "mse"
      ],
      "metadata": {
        "id": "4NzscQS_uhz8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Save the Trained Model:**\n",
        "Saves the trained Random Forest Regressor model (Model5) using the pickle module. The model is serialized and stored in a file named \"WS_Pred_Model\"."
      ],
      "metadata": {
        "id": "kE64kSTFukqJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "pickle_=open(\"WS_Pred_Model\",\"wb\")\n",
        "pickle.dump(Model5,pickle_)\n",
        "pickle_.close()"
      ],
      "metadata": {
        "id": "RA0w7Nvlul1K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Summary**\n",
        "\n",
        "**Data Preparation:**\n",
        "The dataset is split into features (x) and the target variable (y), which is the \"Weekly_Sales\" column.\n",
        "The data is split into training and testing sets using the train_test_split function from scikit-learn.\n",
        "\n",
        "**Model Training:**\n",
        "Four regression models are trained on the training data:\n",
        "Support Vector Regression (SVR)\n",
        "Decision Tree Regressor\n",
        "Gradient Boosting Regressor\n",
        "Random Forest Regressor\n",
        "\n",
        "**Model Evaluation:**\n",
        "For each model, predictions are made on both the training and testing datasets.\n",
        "The coefficient of determination (R^2) is calculated for both the training and testing sets to evaluate the model's performance.\n",
        "\n",
        "**Mean Squared Error (MSE):**\n",
        "The mean squared error (MSE) is calculated to quantify the average squared difference between the true and predicted values for the target variable.\n",
        "\n",
        "**Model Serialization:**\n",
        "The trained Random Forest Regressor model (Model5) is serialized and saved using the pickle module for future use.\n",
        "\n",
        "Overall, the machine learning part involves training multiple regression models, evaluating their performance, and selecting the best model based on the evaluation metrics. Finally, the best model is saved for deployment or further analysis."
      ],
      "metadata": {
        "id": "4PcxDsHeuyOq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Overall Summary"
      ],
      "metadata": {
        "id": "orIDgib1vS4m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Project Overview:**\n",
        "\n",
        "This project involves the analysis and prediction of weekly sales data for a retail store chain. The dataset includes information about store features, sales, and store types. The project follows a structured approach, including data exploration, preprocessing, visualization, and machine learning modeling.\n",
        "\n",
        "**Key Steps:**\n",
        "\n",
        "***Data Exploration:***\n",
        "The project starts with data exploration to understand the structure and features of the datasets. Exploratory Data Analysis (EDA) techniques such as summary statistics, correlation analysis, and visualization are used to gain insights into the data.\n",
        "\n",
        "***Data Preprocessing:***\n",
        "Data preprocessing involves handling missing values, encoding categorical variables, and feature engineering. Techniques like label encoding, handling outliers, and transforming date variables are applied to prepare the data for modeling.\n",
        "\n",
        "***Machine Learning Modeling:***\n",
        "Several regression models, including Support Vector Regression (SVR), Decision Tree Regressor, Gradient Boosting Regressor, and Random Forest Regressor, are trained on the preprocessed data to predict weekly sales.\n",
        "The performance of each model is evaluated using metrics such as R^2 score and Mean Squared Error (MSE).\n",
        "The best-performing model, Random Forest Regressor, is serialized and saved for future use.\n",
        "\n",
        "**Conclusion:**\n",
        "\n",
        "The project concludes with a summary of the machine learning process and the selection of the best model for predicting weekly sales. The serialized model can be deployed for making predictions on new data or integrated into a production environment for decision-making purposes.\n",
        "\n",
        "Overall, this project demonstrates the application of data analysis and machine learning techniques to solve real-world business problems, such as sales forecasting for retail stores."
      ],
      "metadata": {
        "id": "Qt72f6arvVW5"
      }
    }
  ]
}